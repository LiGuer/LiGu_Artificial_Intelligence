\documentclass{article} 
\usepackage{amsmath}
\usepackage[UTF8]{ctex}
\title{}\date{} \setlength{\parindent}{0pt} \linespread{1.25}
\usepackage{ntheorem}\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry} \geometry{a4paper,left=2cm,right=2cm,top=1cm,bottom=1.5cm}

\begin{document}
\tableofcontents

\section{信息论}
    \subsection{离散信息的度量}
        \subsubsection{自信息}
            \begin{enumerate}
                \item \textbf{自信息}: $I_X(a_i) = - \log\ \mathbb P_X(a_i)$
                \item \textbf{联合自信息}: $I_{XY}(a_i, b_i) = -\usepackage{}log\ \mathbb P_{XY}(a_i, b_j)$
                \item \textbf{条件自信息}: $I_{X|Y}(a_i|b_i) = -\log\ \mathbb P_{X|Y}(a_i|b_j)$
            \end{enumerate}
            \textbf{关系}: $I(xy) = I(x) + I(y|x) = I(y) + I(x|y)$
    
            \textbf{意义}: \textbf{自信息}, (1)表示事件发生前, 事件发生的不确定性; (2)表示事件发生后, 事件所包含的信息量, 是提供给信宿的信息量, 也是解除这种不确定性所需要的信息量. 

        \subsubsection{互信息}
            \begin{enumerate}
                \item \textbf{互信息}: $I_{X;Y}(a_i;b_i) = log\frac{\mathbb P_{X|Y}(a_i|b_j)}{\mathbb P_X(a_i)}$
                \item \textbf{条件互信息}: $I_{X;Y|Z}(a_i;b_i|c_i) = -log\frac{\mathbb P_{X|Y,Z}(a_i|b_j, c_i)}{\mathbb P_{X|Z}(a_i|c_i)}$
            \end{enumerate}

            \textbf{意义}: \textbf{互信息}, 反映了两个不同的随机变量集合中两个随机事件之间的统计关联程度, 表示当y发生后x不确定性的变化，表示由y得到的关于x的信息量;

        \subsubsection{信息熵}
            \begin{enumerate}
                \item \textbf{信息熵}: $H(X) = \underset{\mathbb P(x)}{E}[I(x)] = -\sum\limits_x \mathbb P(x)\log \mathbb P(x)$
                \item \textbf{联合信息熵}: $H(XY) = \underset{\mathbb P(xy)}{E}[I(xy)] = -\sum\limits_x \sum\limits_y \mathbb P(xy) \log \mathbb P(xy)$
                \item \textbf{条件信息熵}: $H(Y|X) = \underset{\mathbb P(xy)}{E}[I(y|x)] = -\sum\limits_x \sum\limits_y \mathbb P(xy) \log \mathbb P(y|x) = \sum\limits_x \mathbb P(x) H(Y|x)$
            \end{enumerate}
            
            \textbf{关联}: (当且仅当$X_i$相互独立, 则等式成立.)
            \begin{itemize}
                \item $H(Y|X) \le H(Y)$: 信息处理中, 条件越多, 熵越小.
                \item $H(X_1...X_n) \le \sum\limits_{i=0}^n H(X_i)$
            \end{itemize}
            
            \textbf{意义}: \textbf{信息熵}: 平均自信息, \\
            (1)在信源输出前，表示信源的平均不确定性;\\
            (2)在信源输出后，表示每个信源符号所提供的平均信息量;\\
            (3)表示信源随机性大小，H(X) 大的信源，随机性大;\\
            (4)当信源输出后，不确定性就解除，熵可看成解除信源不确定性平均所需的信息量。
            
            \textbf{性质}: \textbf{信息熵}, 
            \begin{itemize}
                \item 对称性, $H(p_1, p_2, ..., p_n) = H(p_n, p_{n-1}, ..., p_1)$
                \item 非负性, $H(\boldsymbol p) \ge 0$
                \item $H(XY) = H(x) + H(Y|X) \\ H(X_1 X_2 ... X_n) = H(X) + H(X_2|X_1) + ... + H(X_n|X_1...X_{n-1})$
                \item 有限离散随机变量集合中事件等概率发生时, 熵达到最大.
                \item 随机变量集合中任一事件概率为1, 则熵为0.
            \end{itemize}
            
        \subsubsection{交叉熵}
            \textbf{交叉熵}, 若P和Q为定义在同一概率空间的两个概率测度，定义P相对于Q的散度为: $D(\mathbb P || \mathbb Q)=\sum\limits_x \mathbb P(x) \log \frac{\mathbb P(x)}{\mathbb Q(x)}$
            
            \textbf{性质}:\\
                (1)$D(\mathbb P || \mathbb Q) \ge 0$

        \subsubsection{平均互信息}
            \begin{enumerate}
                \item \textbf{集合&事件互信息}: $I(x ; Y)=\sum\limits_{y} \mathbb P(y \mid x) \log \frac{\mathbb P(y \mid x)}{\mathbb P(y)}$
                \item \textbf{平均互信息}: $I(X ; Y) = \underset{\mathbb P(x)}{E}[I(x ; y)] = \sum\limits_{x} \mathbb P(x) I(x ; Y)=\sum\limits_{x, y} \mathbb P(x) \mathbb P(y \mid x) \log \frac{\mathbb P(y \mid x)}{\sum\limits_{x} \mathbb P(x) \mathbb P(y \mid x)}$
                \item \textbf{平均条件互信息}: $I(X ; Y \mid Z)=\underset{\mathbb P(x y z)}{E}[I(x ; y \mid z)]=\sum\limits_{x, y, z} \mathbb P(x y z) \log \frac{p(x \mid y z)}{p(x \mid z)}$
            \end{enumerate}
            
            \textbf{意义}: \textbf{集合&事件互信息}: 表示由事件x提供的关于集合Y的信息量. \textbf{平均互信息}: 从整体上表示一个随机变量Y所给出的关于另一一个随机变量X的信息量.
            
            \textbf{性质}: $I(x ; Y) \ge 0$
            
            \textbf{关联}: 
                \begin{align*}
                    I(X ; Y) &= H(X)-H(X \mid Y) \\
                    I(X ; Y) &= H(Y)-H(Y \mid X) \\
                    I(X ; Y) &= H(X)+H(Y)-H(X Y)
                \end{align*}
             
    \subsection{连续信息的度量}
        \subsubsection{信息熵}
            \begin{enumerate}
                \item \textbf{熵(差熵)}: $h(X)=-\underset{\mathbb P(x)}{E}[\log \mathbb P(x)]=-\int \mathbb P(x) \log \mathbb P(x) \mathrm{d} x$
                \item \textbf{联合差熵}: $h\left(\boldsymbol{X}^{N}\right)=-\underset{\mathbb P(\boldsymbol{x})}{E}[\log \mathbb P(\boldsymbol{x})]=-\int_{\boldsymbol{x}} \mathbb P(\boldsymbol{x}) \log \mathbb P(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$
                \item \textbf{条件差熵}: $h(X \mid Y)=-\underset{\mathbb P(x y)}{E}[\log \mathbb P(x \mid y)]=-\iint \mathbb P(x y) \log \mathbb P(x \mid y) \mathrm{d} x \mathrm{~d} y$
                \item \textbf{交叉熵}: $D(\mathbb P || \mathbb Q)=\int \mathbb P(x) \log \frac{\mathbb P(x)}{\mathbb Q(x)} \mathrm{d} x$
            \end{enumerate}
            
            \textbf{连续最大熵定理}:
                \begin{itemize}
                    \item 限峰值最大熵: 幅度受限的随机变量，当均匀分布时有最大的熵.
                    \item 限功率最大熵: 平均功率受限的随机变量，当高斯分布时有最大的熵.
                \end{itemize}
            
        \subsubsection{\textbf{eg.}离散时间Gauss信源的熵}
            \begin{enumerate}
                \item \textbf{1维Gauss随机变量集的熵}: $h(X)=\frac{1}{2} \log \left(2 \pi e \sigma^{2}\right)$
                \item \textbf{多维独立Gauss随机变量集的熵}: $h\left(\boldsymbol{X}^{N}\right)=\frac{N}{2} \log \left[2 \pi \mathrm{e}\left(\sigma_{1}^{2} \sigma_{2}^{2} \cdots \sigma_{n}^{2}\right)^{\frac{1}{N}}\right]$
                \item \textbf{多维相关Gauss随机变量集的熵}: $h\left(\boldsymbol{X}^{N}\right)=\frac{N}{2} \log \left[2 \pi \mathrm{e} |\boldsymbol \Sigma|^{\frac{1}{N}}\right]$
            \end{enumerate}
            
        \subsubsection{平均互信息}
            \textbf{平均互信息}: $I(X ; Y)=\operatorname{Sup}_{P, Q} I\left([X]_{P} ;[Y]_{Q}\right)$
            
            \textbf{关联}:
                \begin{align*}
                    I(X ; Y) &= h(X)-h(X \mid Y) \\
                    I(X ; Y) &= h(Y)-h(Y \mid X) \\
                    I(X ; Y) &= h(X)+h(Y)-h(X Y)
                \end{align*}
                
        \subsubsection{离散集 \& 连续集的互信息}
            \begin{enumerate}
                \item \textbf{离散集 \& 连续集的互信息}: $I(x ; y)=\log \frac{q(x \mid y)}{p(x)}=\log \frac{p(y \mid x)}{q(y)}$
                \item \textbf{离散集 \& 连续集的平均互信息}: $I(X ; Y)=\underset{p(x) p(y \mid x)}{E}\left[\log \frac{p(y \mid x)}{q(y)}\right]=\sum_{x} p(x) \int p(y \mid x) \log \frac{p(y \mid x)}{q(y)} \mathrm{d} y$
            \end{enumerate}
            
            
    \subsection{信源: 离散信源 \& 连续信源}
        \subsubsection{离散无记忆信源}
            \textbf{离散无记忆信源}: $ \left(\begin{array}{l} X \\ P \end{array}\right)
                     = \left(\begin{array}{lll}
                        a_{1} & \cdots & a_{n} \\ p\left(a_{1}\right) & \cdots & p\left(a_{n}\right)
                    \end{array}\right) \quad;
                    p\left(a_{i}\right) \geqslant 0, \sum_{i=1}^{n} p\left(a_{i}\right)=1$
                
            \textbf{离散无记忆信源的熵}: $H(X) = -\sum p \log p$

            \textbf{离散无记忆信源N次扩展源的熵}: $H(X^N) = N\ H(X)$
            
        \subsubsection{离散有记忆信源}

            \textbf{离散平稳有记忆信源的熵}: $H(X^N) = H(X_1 X_2 ... X_N) = H(X_1) + H(X_2 | X_1) + ... + H(X_N | X_1 ... X_{N-1}) \le N\ X(X_1)$

            \textbf{信源的极限符号熵}: $H_\infty (X) = \lim\limits_{x \to \infty} \frac{1}{N} H(X^N)$

        \subsubsection{相关性 \& 剩余度}
            \textbf{信源效率}: $\eta = \frac{H_\infty}{H_0}$
            \textbf{信源剩余度}: $\eta = 1 - \frac{H_\infty}{H_0}$
            
    \subsection{信源编码}
        \textbf{定长码}
            \textbf{性质}:
                \textbf{典型序列}: 序列中所有符号出现的次数满足大数定律的序列称为典型序列，否则为非典型序列。

                \textbf{定长码编码速率}: $R^{\prime}=\frac{l \log r}{N}$

                \textbf{定长码编码效率}: $\eta=\frac{H(X)}{R^{\prime}}=\frac{N H(X)}{l \log r}$
                
                \textbf{定长码序列长度}: $N \geqslant \frac{\sigma^{2}}{\delta^{2} \varepsilon}=\left(\frac{\eta}{1-\eta}\right)^{2} \cdot \frac{\sigma^{2}}{H^{2}(X) \varepsilon}$

        \textbf{变长码}  
            \textbf{性质}:
                \textbf{异前置码}: 在异前置码树中，只有叶节点对应码字，从而要求对应码字的端点与根之间不能有其他节点作为码字，端点也不能向上延伸再构成新码字.
                
            \textbf{例子}:
                \textbf{Haffman编码}:
                    \textbf{编码方法}:
                        (1)将信源概率分布按大小依递减次序排列;合并两概率最小者，得到新信源:并分配0，1符号。
                        (2)新信源若包含两个以上符号返回(1)， 否则到(3)。
                        (3)从最后- -级向前， 按顺序写出每个信源符号所对应的码字。
                    \textbf{特点}:
                        最优码, 编码结果不唯一.
                    
                    

        
    \subsection{信道: 信道容量}
    
    \subsection{信道编码}
        
\end{document}
