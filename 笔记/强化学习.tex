\documentclass{article} 
\usepackage{amsmath}
\usepackage[UTF8]{ctex}
\title{强化学习}\date{} \linespread{1.25}
\usepackage{ntheorem}\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry} \geometry{a4paper,left=2cm,right=2cm,top=1cm,bottom=1.5cm}
\usepackage{paralist}
\let\itemize\compactitem
\let\enumerate\compactenum

\begin{document}
\tableofcontents

\section{强化学习-核心}
    \subsection{定义}
        \textbf{定义}: 在交互中学习, 以实现一个目标的计算性方法. 即, 让\textbf{智能体}去和未知的\textbf{环境}互动, 学习如何动作, 来获得最大累计奖励.
        
    \subsection{智能体 \& 环境}
        \begin{itemize}
            \item \textbf{智能体}: 学习和决策者.
            \item \textbf{环境}: 与智能体交互的, 除智能体之外的一切事物.
        \end{itemize}
        
        \subsubsection{智能体-环境的边界如何确定?}
            界限并没有确定性的定义, 而是根据实际问题来.遵循的一般规则是, 任何不能由智能体任意更改的内容, 都被视为是环境的一部分.
            
            \textbf{eg.} 奖励可能在智能体内计算, 但被认为是环境的, 因为奖励定义了智能体面临的任务, 因此必须超出智能体任意改变的能力.


    \subsection{状态, 动作, 奖励 $S, A, R$}
    3类信号:
        \begin{displaymath} \left\{ \begin{array}{l}
            \text{状态}\ S\\
            \text{动作}\ A\\
            \text{奖励}\ R
        \end{array} \right. \end{displaymath}
        
        \subsubsection{累计奖励 $\sum R$}
            累积奖励: 获得最大的累积奖励是智能体的唯一目标.
                $$G_t= \sum_{i = t+1}^{T} R_i$$
            为了避免$T \to \infty, G_t \to \infty$, 引入折扣$\gamma \in [0,1]$, 期望折扣回报:
                $$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}$$
        
        \textbf{核心要素}:
        \begin{displaymath} \left\{ \begin{array}{llll}
            S \to A & \text{策略} & \text{定义了智能体的行为方式, 智能体在当前状态如何动作}\\
            A \to S & \text{环境模型}(A_t,S_t\to S_{t+1}) & \text{环境对于动作的反应, 如何转移到下一状态.(可选)}\\
            S \to R & \text{奖励函数} & \text{定义了智能体的目标, 当前环境对智能体反馈的奖励.}\\
            S \to \sum R & \text{价值函数} & \text{当前状态下的长期累计收益的期望.}
        \end{array} \right. \end{displaymath}

        \begin{displaymath} \left\{ \begin{array}{lllll}
            S \leftrightarrow A: \quad & A = F(S) & (\text{策略函数})    & S = F(A) & (\text{环境更新})\\
            S \leftrightarrow R: \quad & R = F(S) & (\text{价值函数})    & S = F(R) & (\text{无})\\
            A \leftrightarrow R: \quad & A = F(R) & (\text{无})    & R = F(A) & (\text{无})
        \end{array} \right. \end{displaymath}
        
    \subsection{强化学习\&有/无监督学习}
        强化学习是学习的第三类范式, 其与有/无监督学习不相同.
    
        \subsubsection{与有监督学习的异同}
            \textbf{监督学习},是从外部监督者提供的"样本-标记对"的训练集中学习的一种方式. 目的是学习如何推断和概括, 以拥有泛化(即, 对未见过的样本也能正确判断)的能力.
        
            \begin{displaymath} \left\{ \begin{array}{l}
                \text{状态 --- 样本}\\
                \text{动作 --- 标记}\\
                \text{策略 --- 分类/回归器}
            \end{array} \right. \end{displaymath}
            
            \textbf{异同点}: 相似之处,如上式, 但强化学习并没有明确的"样本---标记"对来提供训练. 在交互问题的未知场景中, 不能即时获得当前"状态S---动作A"关联的答案, 也无法即时获得"状态S---动作A"选择的好坏, 其产生的影响只能在未来长期累积奖励中反映出来.因此, 单靠监督学习不足以实现"从互动中学习". 强化学习, 可以看作是一种具有延迟标记信息的监督学习.
            
            
        
        \subsubsection{与无监督学习的异同}
            \textbf{无监督学习}, 是寻找蕴藏在未标记的数据里的结构特征.

            \textbf{异同点}: 强化学习是最大化累计奖励, 而非寻找隐藏的结构, 因此, 无监督学习不包含强化学习.

        
        
    \subsection{多臂赌博机问题 $S = \{s_0\}$}
        \subsubsection{探索-利用\ 平衡}
        
        \textbf{平稳问题}: 奖励概率不随时间变化.
        
        参数更新的递推形式:
        \begin{align*}
            Q_{n+1} 
            &= \frac{1}{n} \sum_{i = 0}^n R_i\\
            &= \frac{1}{n} (R_n + \sum_{i = 0}^{n-1} R_i)\\
            &= \frac{1}{n} (R_n + (n-1) Q_n)\\
            &= Q_n + \frac{1}{n} (R_n - Q_n)
        \end{align*}
        \begin{align*}
            Estimate_{New} 
            &\gets Estimate_{Old} + StepSize\cdot (Target - Estimate_{Old})\\
            &= Estimate_{Old} + StepSize\cdot Error
        \end{align*}
        
        \textbf{非平稳问题}: 奖励概率随时间变化. 对最近的奖励给予更多的重视比对很久以前的奖励给予更多的重视是有道理的。最常用的方法之一是使用恒定步长参数。
            $$Q_{n+1} = Q_n + \alpha (R_n - Q_n)$$
            
        \subsubsection{算法1: $\epsilon$贪心}
            \begin{displaymath} A = \left\{ \begin{array}{ll}
                \arg\max_a\ Q(a) & \mathbb{P}(\text{贪心}) = \epsilon\\
                \text{a random action} & \mathbb{P}(\text{随机}) = 1 - \epsilon
            \end{array} \right. \end{displaymath}
            
        \subsubsection{算法2: 上置信界算法}
            始终选择置信度上界最大的动作作为选择策略.
            $$A_{t} = \underset{a}{\operatorname{argmax}}\left[Q_{t}(a)+c \sqrt{\frac{\ln t}{N_{t}(a)}}\right]$$
            \textbf{Proof}:\\
                Gauss分布下的Chernoff边界:
                    $$\mathbb{P}(\mu \geq \bar{\mu}+\epsilon) \leq \exp \left(\frac{-n \epsilon^{2}}{2}\right)$$
                令$\delta = e^{-n \epsilon / 2}$, 即 $\epsilon = \sqrt{\frac{2}{n} \ln \frac{1}{\sigma}}$
                \begin{align*}
                    \Rightarrow & \mathbb{P}\left(\mu \ge \bar{\mu} + \sqrt{\frac{2}{n} \ln \frac{1}{\sigma}} \right) \le \sigma\\
                    \Rightarrow & \mathbb{P}\left(\bar{\mu} \in \left[\mu - \sqrt{\frac{2}{n} \ln \frac{1}{\sigma}}, \mu +  \sqrt{\frac{2}{n} \ln \frac{1}{\sigma}} \right] \right) \ge 1 - 2 \sigma
                \end{align*}
                
        \subsubsection{算法3: Softmax算法}
            \textbf{soft-max分布}:
                $$\operatorname{Pr}\left\{A_{t}=a\right\} = \frac{e^{H_{t}(a)}}{\sum_{b=1}^{k} e^{H_{t}(b)}} = \mathbb{P}_{\pi,t}(a)$$
    
    \subsection{Markov决策过程 $S_t \to S_{t+1}$}
        Markov性质: 当前状态$S_t$只与上一时刻状态$S_{t-1}$有关, 与上一时刻之前的任何状态$S_{t-k \mid_{k > 1}}$无关.
            $$\mathbb{P}\left(S_{t+1}=s \mid S_{1}=s_{1}, \ldots, S_{t}=s_{t}\right)=\mathbb{P}\left(S_{t+1}=s \mid S_{t}=s_{t}\right)$$
        四元组:
            $$<S, A, P, R>$$
            \begin{displaymath} \left\{ \begin{array}{l}
                S: \text{状态集.}\\
                A: \text{动作集.}\\
                P: \text{状态转移概率.}\\
                R: \text{奖励函数}.
            \end{array} \right. \end{displaymath}
            
        动态概率:
            $$\mathbb{P}(s', r \mid s, a)$$
            $$\sum_{s' \in S} \sum_{r \in R} \mathbb{P}(s', r \mid s, a) = 1$$
        可导出,\\
        状态转移概率:
            $$\mathbb{P}(s' \mid s,a) = \sum_{r \in R} \mathbb{P}(s', r \mid s, a)$$
        奖励分布:
            $$r(s, a) = \sum_{r \in R} r \sum_{s \in S} \mathbb{P}(s', r \mid s, a)$$
            $$r(s, a, s') = \sum_{r \in R} r \frac{\mathbb{P}(s', r \mid s, a)}{\mathbb{P}(s' \mid s, a)}$$
            
        \textbf{预测问题}：对给定策略的评估过程。已知一个Markov决策过程以及策略, 目标是求解基于该策略的价值函数, 即处于每个状态下能够获得的奖励是多少。
        
        \textbf{控制问题}：寻找一个最优策略的过程。已知一个Markov决策过程但是策略未知, 求解最优价值函数和最优策略。

        
    \subsection{价值\&策略函数 $S-A-R$}
        \subsubsection{价值函数 $R = V(S)$}
            价值函数: 评价智能体在给定状态$S_t$下预期的累计奖励. (Bellman方程)
            \begin{align*}
                V(s) 
                &= \mathbb{E} \left[G_t \mid S_t=s\right] \tag{定义}\\
                &= \mathbb{E} \left[R_{t+1} + \gamma G_{t+1} \mid S_t=s\right] \tag{代换}\\
                &= \sum_{a \in A} \mathbb{P}_\pi(a \mid s) \left( \sum_{s' \in S} \sum_{r \in R} \mathbb{P}(s', r | s, a)\cdot \left(r + \gamma \mathbb{E} \left[G_{t+1} \mid S_{t+1} = s'\right]\right) \right) \tag{期望公式}\\
                &= \sum_{a \in A} \mathbb{P}_\pi(a \mid s) \left( \sum_{s' \in S} \sum_{r \in R} \mathbb{P}(s', r | s, a)\cdot \left(r + \gamma V(s') \right) \right) \tag{代换}\\
                &= \mathbb{E} \left[R_{t+1} + \gamma V(S_{t+1}) \mid S_t=s\right]
            \end{align*}

            (1) 立即回报$R_{t+1}$\\
        	(2) 后继状态的折扣价值函数$\gamma \cdot V(S_{t+1})$
        
        \subsubsection{动作-价值函数 $R_A = Q(S,A)$}
            \begin{align*}
                Q(s,a) 
                &= \mathbb{E}\left[G_t \mid S_t=s, A_t = a\right]\\
                &= \mathbb{E}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s, A_t = a\right]\\
                &= \mathbb{E}_{s^{\prime}}\left[r+\gamma Q\left(s^{\prime}, a^{\prime}\right) \mid s, a\right]
            \end{align*}
            
        \subsubsection{策略函数 $A = \Pi(S)$}
            \begin{itemize}
                \item 确定性策略: $a = \pi(s)$
                \item 随机性策略: $\mathbb{P}_\pi(a \mid s)$
            \end{itemize}
            
        \subsubsection{优化}
    
\section{基于价值函数}
    
    \subsection{动态规划}
        \subsubsection{策略迭代 = 策略评估$V_{\pi}$ + 策略改进$\pi \to \pi'$}
            \textbf{策略评估}: 求当前策略下的累计奖励$V_{\pi}$.
            
                解析解: (计算复杂度$|S|^3$)
                \begin{align*}
                    V_{\pi}(s)
                    &=\mathbb{E}_{\pia}\left[R_{t+1}+\gamma V_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
                    &= \sum_{a} \pi(a \mid s) \sum_{s',r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma V_{\pi}\left(s^{\prime}\right)\right]\\
                    \Rightarrow V_{\pi} &= R_{\pi} + \gamma P_{\pi}V_{\pi} \tag{矩阵式}\\
                    \Rightarrow V_{\pi} &= (I - \gamma P_{\pi})^{-1} R_{\pi} \tag{解析解})
                \end{align*}
                
                迭代计算数值解: $\lim\limits_{k \to \infty} V_k = V_{\pi}$
                \begin{align*}
                    V_{k+1}(s) & = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma V_{k}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
                    &= \sum_{a} \pi(a \mid s) \sum_{s^{\prime} r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma V_{k}\left(s^{\prime}\right)\right]
                \end{align*}
                
            \textbf{策略改进}: 找到累计奖励更多的策略$\pi'$.
                $$\forall s \in S,\ V_\pi(s) \le Q(s, \pi'(s)) \quad \Rightarrow \quad V_\pi(s) \le V_{\pi'}(s)$$
                贪心策略: $\forall s \in S,\ \pi' = \arg\max_a\ q_\pi (s,a)$
                
            \textbf{策略迭代} = 策略评估$V_{\pi}$ + 策略改进$\pi \to \pi'$
                $$V_{\pi} \Rightarrow (\pi \to \pi') \Rightarrow V_{\pi} \Rightarrow (\pi \to \pi') \Rightarrow ... \Rightarrow \{\pi^*, V^*\}$$
        
        \subsubsection{广义策略迭代}
            让策略评估和策略提升进行交互, 且独立于间隔和其他二者的细节。
    
    \subsection{Monte Carlo方法}
        \textbf{深度优先搜索}.
    
        我们不假设对环境有完全了解。蒙特卡罗方法只需要从与环境的实际或模拟交互中体验状态、动作和奖励的样本序列。只有在一个片段结束后, 价值评估和策略才会更新。因此, Monte Carlo方法是逐段递增的, 而不是一步一步在线的.
        
        对值函数的估计:
            $$V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}-V\left(S_{t}\right)\right)$$

    \subsection{时序差分学习}
        时序差分学习是蒙特卡罗思想和动态规划思想的结合。与蒙特卡罗方法一样, 时序差分方法可以直接从原始经验中学习, 而无需环境动力学模型。与动态规划一样, 时序差分方法部分基于其他学习的估计更新估计, 而无需等待最终结果（它们是自举的）.
            $$V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)$$
        
        \subsubsection{时间差分预测}
    
    	\subsubsection{Q-Learning}
    	    $$\Delta Q(s,a) = \alpha\ ( R + \gamma \cdot max\ Q(s',:) - Q(s,a))$$
    	    
    	\subsubsection{Sarsa}
    	    $$\Delta Q(S_{t},A_{t}) = \alpha\ ( R_{t+1} + \gamma\ Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$$
    	
    	\subsubsection{Deep Q Networks = Q-Learning + 神经网络}
    	    记忆库 (用于重复学习)
            神经网络计算 Q 值
    	
            损失函数:
                $$L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot)}\left[\left(y_{i}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]$$
                $$\nabla_{\theta_{i}} L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot) ; s^{\prime} \sim \mathcal{E}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(s, a ; \theta_{i}\right)\right]$$


\section{基于策略函数}
    \subsection{策略函数 $A = \Pi(S)$}
        \begin{itemize}
            \item 随机性策略: $\mathbb{P}_\pi(a \mid s)$
            \item 确定性策略: $a = \pi(s)$
        \end{itemize}

    \subsection{策略梯度}
        \textbf{策略梯度}, 是一种参数化策略的学习框架, 主要解决连续动作空间的强化学习问题,  通过最大化性能度量$J(\theta)$并梯度上升来更新$\theta$, 且 动作选择由策略函数决定, 而不再直接依赖价值函数, 无论是否同时学习一个近似的价值函数. 其中策略函数$\pi(a|s)$应当对$\theta$可导.
            $$\mathbb{P}_\pi (a \mid s, \theta)$$
            $$ \theta_t = \theta_{t+1} + \alpha  \widehat{\Delta J(\theta_t)}$$
        
        \textbf{性能度量函数}, .设性能度量函数即是价值函数$J(\boldsymbol{\theta}) = V(s)$, 则梯度有:
            \begin{align*}
                \nabla J(\boldsymbol{\theta}) &= \nabla_\theta V(s) \\
                &\propto \sum\limits_{s' \in S} \mathbb P_{\mu, \pi}(s') \sum\limits_{a \in A} Q_{\pi}(s', a) \nabla_{\theta} \mathbb{P}_\pi(a \mid s', \boldsymbol{\theta}) \\
                & \propto \underset{s' \sim \mathbb P_{\mu, \pi} \atop a \sim \pi}{\mathbb E} \left[Q_{\pi}(s', a)\  \nabla_\theta \ln \mathbb{P}_\pi (a \mid s') \right]
            \end{align*}
        
        \textbf{证明}:
            \begin{align*}
                \nabla_\theta V(s)
                &= \nabla_\theta\ \mathbb E[G_t|S_t = s] \tag{价值函数定义}\\
                &= \nabla_\theta \left(\sum_{a \in A} \mathbb{P}_\pi (a \mid s)\ Q(s,a) \right) \tag{期望公式}\\
                &= \sum_{a \in A} \nabla_\theta \left( \mathbb{P}_\pi (a \mid s)\ Q(s,a) \right) \tag{分配律}\\
                &= \sum_{a \in A} \left( Q(s,a)\ \nabla_\theta \mathbb{P}_\pi (a \mid s) + \mathbb{P}_\pi (a \mid s)\ \nabla_\theta Q(s,a) \right) \tag{微分乘法法则}\\
                &= \sum_{a \in A} \left( Q(s,a)\ \nabla_\theta \mathbb{P}_\pi (a \mid s) + \mathbb{P}_\pi (a \mid s)\ \nabla_\theta \sum_{s', r} \mathbb{P}(s', r | s, a) \left(r + \gamma V(s') \right) \right) \tag{代入}\\
                &= \sum_{a \in A} \left( Q(s,a)\ \nabla_\theta \mathbb{P}_\pi (a \mid s)+ \mathbb{P}_\pi (a \mid s)\ \sum_{s', r} \mathbb{P}(s', r | s, a) \gamma \nabla_\theta V(s') \right) \tag{分配律}\\
                &= \sum_{a \in A} Q(s,a)\ \nabla_\theta \mathbb{P}_\pi (a \mid s) + \sum_{s'} \gamma \nabla_\theta V(s') \sum_{a \in A} \mathbb{P}_\pi (a \mid s)\sum_{r} \mathbb{P}(s', r | s, a)  \tag{加法性质}\\
                &= \sum_{a \in A} Q(s,a)\ \nabla_\theta \mathbb{P}_\pi (a \mid s) + \sum_{s'} \gamma \nabla_\theta V(s') \sum_{a \in A} \mathbb{P}(s', a| s)  \tag{条件概率公式}\\
                &= \sum_{a \in A} Q(s,a)\ \nabla_\theta \mathbb{P}_\pi (a \mid s) + \sum_{s'} \gamma \mathbb{P}(s'| s)  \nabla_\theta V(s') \tag{概率求和}\\
                &= \sum_{s' \in S} \sum_{t=0}^{\infty} 
                \gamma^t \mathbb P^{(t)}_\pi(s' | s) \sum_{a \in A} Q_{\pi}(s', a) \nabla_\theta \mathbb{P}_\pi (a \mid s')  \tag{递推$ \nabla_\theta V(s')$展开, $\mathbb P^{(t)}$是t步转移概率}\\
                &= \left(\sum_{s''\in S} \sum\limits_{t=0}^{\infty} \gamma^t \mathbb P_\pi^{(t)}(s''| s)\right) \cdot \sum\limits_{s' \in S} \frac{\sum\limits_{t=0}^{\infty} \gamma^t \mathbb P_\pi^{(t)}\left(s'| s\right)}{\sum\limits_{s''\in S} \left(\sum\limits_{t=0}^{\infty} \gamma^t \mathbb P_\pi^{(t)}\left(s''| s\right)\right)} \sum_{a \in A} Q_{\pi}(s', a) \nabla_\theta \mathbb{P}_\pi (a \mid s')  \tag{提项, 目的是将$\sum\limits_{t=0}^{\infty} \mathbb P_\pi^{(t)}\left(s'| s\right)$归一化}\\
                &= \alpha \sum\limits_{s' \in S} \mathbb P_{\mu, \pi}(s') \sum_{a \in A} Q_{\pi}(s', a) \nabla_\theta \mathbb{P}_\pi (a \mid s')  \tag{\small 简写,$\left(\sum\limits_{s''\in S} \sum\limits_{t=0}^{\infty} \gamma^t \mathbb P_\pi^{(t)}(s''| s)\right)$简为系数, $\mathbb P_{\mu, \pi}(s')$是状态在策略$\pi$下整个时间序列出现的概率}\\
                &= \alpha \underset{s' \sim \mathbb P_{\mu, \pi}}{\mathbb E} \left[ \sum_{a \in A} Q_{\pi}(s', a) \nabla_\theta \mathbb{P}_\pi (a \mid s') \right] \tag{期望形式}\\
                &= \alpha \underset{s' \sim \mathbb P_{\mu, \pi}}{\mathbb E} \left[ \sum_{a \in A} \mathbb{P}_\pi (a \mid s') Q_{\pi}(s', a) \frac{\nabla_\theta \mathbb{P}_\pi (a \mid s')}{\mathbb{P}_\pi (a \mid s')} \right] \tag{提项$\mathbb{P}_\pi (a \mid s')$, 方便后面对$a$写期望}\\
                &= \alpha \underset{s' \sim \mathbb P_{\mu, \pi} \atop a \sim \pi}{\mathbb E} \left[Q_{\pi}(s', a)\  \nabla_\theta \ln \mathbb{P}_\pi (a \mid s') \right] \tag{期望形式, 微分公式}\\
            \end{align*}
            (证毕.)
            
        \textbf{优点}, 能在一个连续区间内挑选动作
        
        \textbf{缺点}:
            即使通过PG学习得到了随机策略之后, 在每一步行为时, 我们还需要对得到的最优策略概率分布进行采样, 才能获得action的具体值；而action通常是高维的向量, 比如25维、50维, 在高维的action空间的频繁采样, 无疑是很耗费计算能力的； 在PG的学习过程中, 每一步计算policy gradient都需要在整个action space进行积分,这个积分我们一般通过Monte Carlo 采样来进行估算, 需要在高维的action空间进行采样, 耗费计算能力。

            
    \subsubsection{确定性策略梯度} 
        \textbf{确定性策略梯度}: 采用确定性策略函数.
        
        \textbf{eg.} 策略函数常用形式
        \begin{displaymath} a = \pi(s) = \left\{ \begin{array}{lr}
            \underset{a}{\operatorname{argmax}}\ Q_\pi(s, a)    & (\text{max})\\
            \underset{a}{\operatorname{argmax}}\ \frac{e^{h(s, a, \theta)}}{\sum_{b} e^{h(s, b, \theta)}}    & (\text{soft-max})
        \end{array} \right. \end{displaymath}
        
        \textbf{确定策略梯度定理}: 确定性策略梯度定理乍一看不像随机版本（方程式2）。然而，我们现在证明，对于一大类随机策略，包括许多凸点函数，确定性策略梯度确实是随机策略梯度的一个特殊（极限）情况。我们通过确定性策略πθ：S来参数化随机策略πθ，σ→ A和方差参数σ，使得对于σ=0，随机策略等价于确定性策略πμθ，0≡ µθ. 然后我们将其表示为σ→ 0随机策略梯度收敛于确定性梯度（证明和技术条件见附录C）。
            $$\nabla_{\theta} J\left(\mu_{\theta}\right) =\left.\int_{\mathcal{S}} \rho^{\mu}(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu_{\theta}(s)} \\=\mathbb{E}_{s \sim \rho^{\mu}}\left[\left.\nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu_{\theta}(s)}\right]$$ 
            
            $$\lim _{\sigma \downarrow 0} \nabla_{\theta} J\left(\pi_{\mu_{\theta}, \sigma}\right)=\nabla_{\theta} J\left(\mu_{\theta}\right)$$

\section{基于策略+价值}
    \subsection{Actor-Critic}
        \textbf{Actor-Critic}, \textit{是一种基于策略梯度定理的广泛使用的架构}, 结合了 策略梯度算法 (Actor) + 时序差分算法 (Critic) . Actor-Critic由两部分组成:
    
        \begin{itemize}
            \item \textbf{Actor}: 基于策略函数$\pi_\theta(s)$选择行为, 根据 Critic 的评分更新随机策略函数. \\ \textit{通过$\nabla_{\theta} J\left(\pi_{\theta}\right)$的随机梯度上升调整随机策略$\pi_\theta(s)$ 的参数$\theta$。代替方程2中未知的真作用值函数$Q_\pi (s, a)$, 使用作用值函数$Q_w (s, a)$, 参数向量为$w$。}
            
            \item \textbf{Critic}: 基于 Actor 的行为评判行为的得分, 估计动作-价值函数$Q_w (s, a) \simeq  Q_\pi (s, a)$. \\ \textit{使用适当的策略评估算法, 就像时序差分学习一样。通常, 用函数逼近器$Q_w (s, a)$代替真作用值函数$Q_\pi (s, a)$可能会引入偏差。然而, 如果函数逼近器兼容, 使得(1) $Q_w (s, a) = \nabla \theta log \pi_\theta (a | s)>w$和 (2) 选择参数w以最小化均方误差$\epsilon^2 (w)=E_{s \sim \rho_\pi, a \sim \pi_\theta} [ (Q_w (s, a)− Q_\pi (s, a))^2]$, 则不存在偏差$\nabla_{\theta} J\left(\pi_{\theta}\right)=\mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{w}(s, a)\right]$. 更直观地说，条件(1)表明相容函数逼近器在随机策略的“特征”中是线性的，$\nabla \theta log \pi_\theta (a | s)$和条件(2)要求参数是线性回归问题的解，该线性回归问题根据这些特征估计$Q_\pi(s，a)$。在实践中，条件(2)通常被放宽，以支持通过时间差分学习更有效地估计价值函数的策略评估算法, 事实上，如果同时满足近似值(1)和(2)，则整个算法相当于根本不使用批评家. }
        \end{itemize}
    
        \begin{align*}
            \boldsymbol{\theta}_{t+1} 
            &= \boldsymbol{\theta}_{t}+\alpha\left(G_{t: t+1}-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}_{t}\right)} \\
            &=\boldsymbol{\theta}_{t}+\alpha\left(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}_{t}\right)} \\
            &=\boldsymbol{\theta}_{t}+\alpha \delta_{t} \frac{\nabla \pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}_{t}\right)}
        \end{align*}
        
        \textbf{优势}: 引入了时序差分算法, 因此可以进行单步更新, 比传统 Policy Gradient 训练更快.
        
        \textbf{劣势}: 取决于 Critic 的价值判断, 但是 Critic 难收敛, 再加上 Actor 的更新, 就更难收敛. Deep Deterministic Policy Gradient融合了 DQN 的优势, 解决了收敛难的问题.  
        
        Actor 在运用 随机梯度算法 进行 梯度上升 时, 由 Critic 来告诉他, 这次的 梯度上升 是不是一次正确的上升, 如果这次的得分不好, 那么就不要上升那么多. Actor 想要最大化期望的奖励, 在 Actor Critic 算法中, 我们用 比平时好多少 (TD error) 来当做奖励. Critic 的更新很简单, 就是像 Q-learning 那样更新现实和估计的误差 (TD error) 就好了.
        
    \subsection{确定性 Actor-Critic}
        \textit{现在我们考虑如何将策略梯度框架扩展到确定性策略。我们的主要结果是一个确定性策略梯度定理，类似于前一节中介绍的随机策略梯度定理。我们提供了几种方法来推导和理解这个结果。首先，我们在确定性策略梯度的形式背后提供了一种非正式的直觉。然后从第一性原理出发，给出了确定性策略梯度定理的形式化证明。最后，我们证明了确定性策略梯度定理实际上是随机策略梯度定理的一个极限情况。证据的细节将推迟到附录中。}
    
    
        \textit{现在，我们使用确定性策略梯度定理来推导策略上和策略下的参与者批评算法。我们从最简单的案例开始——关于策略更新，使用一个简单的Sarsa批评者——以便尽可能清楚地说明想法。然后，我们考虑关闭策略的情况下，这一次使用一个简单的Q-学习批评家来说明关键思想。由于函数逼近器引入的偏差以及非策略学习引起的不稳定性，这些简单算法在实践中可能存在收敛问题。然后，我们转向一种更具原则性的方法，使用兼容函数近似和梯度时间差分学习}
        
        \subsubsection{On-Policy Deterministic Actor-Critic}
            \begin{align*}
                \delta_{t} &=r_{t}+\gamma Q^{w}\left(s_{t+1}, a_{t+1}\right)-Q^{w}\left(s_{t}, a_{t}\right) \\
                w_{t+1} &=w_{t}+\alpha_{w} \delta_{t} \nabla_{w} Q^{w}\left(s_{t}, a_{t}\right) \\
                \theta_{t+1} &=\theta_{t}+\left.\alpha_{\theta} \nabla_{\theta} \mu_{\theta}\left(s_{t}\right) \nabla_{a} Q^{w}\left(s_{t}, a_{t}\right)\right|_{a=\mu_{\theta}(s)}
            \end{align*}
            

        
        \subsubsection{Off-Policy Deterministic Actor-Critic}
            \begin{align*}
                \delta_{t} &=r_{t}+\gamma Q^{w}\left(s_{t+1}, \mu_{\theta}\left(s_{t+1}\right)\right)-Q^{w}\left(s_{t}, a_{t}\right) \\
                w_{t+1} &=w_{t}+\alpha_{w} \delta_{t} \nabla_{w} Q^{w}\left(s_{t}, a_{t}\right) \\
                \theta_{t+1} &=\theta_{t}+\left.\alpha_{\theta} \nabla_{\theta} \mu_{\theta}\left(s_{t}\right) \nabla_{a} Q^{w}\left(s_{t}, a_{t}\right)\right|_{a=\mu_{\theta}(s)}
            \end{align*}
            
            随机非策略参与者-批评家算法通常对参与者和批评家都使用重要性抽样。然而，由于确定性策略梯度消除了动作上的积分，我们可以避免在参与者中进行重要抽样；通过使用Q-learning，我们可以避免批评家中的重要抽样
            
        \textbf{参考文献}: [1] Author A , Address A . Deterministic Policy Gradient Algorithms. 2002.
            
    \subsection{相容函数逼近}
        
    \subsection{Deep Deterministic Policy Gradient}
        \textbf{Deep Deterministic Policy Gradient}: Deep: Actor Critic + Deep Q Networks;\quad Deterministic: 策略函数采用"确定性策略函数";
    
        \textit{将深度Q学习成功背后的理念应用于持续行动领域。提出了一种基于确定性策略梯度的 Actor批评、无模型算法，该算法可以在连续动作空间上运行。使用相同的学习算法、网络结构和超参数，我们的算法稳健地解决了20多个模拟物理任务，包括经典问题，如手推车摆动、灵巧操作、腿部运动和汽车驾驶。我们的算法能够找到性能与规划算法相比具有竞争力的策略，并且能够完全访问域及其派生的动态。我们进一步证明，对于许多任务，该算法可以“端到端”学习策略：直接从原始像素输入。}

        
        Policy gradient 在连续动作上进行动作的筛选 . 一种使用 Actor Critic 结构, 但是输出的不是行为的概率, 而是具体的行为, 用于连续动作 (continuous action) 的预测. DDPG 结合了之前获得成功的 DQN 结构, 提高了 Actor Critic 的稳定性和收敛性.
        
        \textbf{深度学习的作用, 对于DPG的关键改进}:
        \begin{enumerate}
            \item 使用卷积神经网络来模拟策略函数和Q函数, 并用深度学习的方法来训练, 证明了在RL方法中, 非线性模拟函数的准确性和高性能、可收敛；而DPG中, 可以看成使用线性回归的机器学习方法：使用带参数的线性函数来模拟策略函数和Q函数, 然后使用线性回归的方法进行训练。
            \item experience replay memory的使用：actor同环境交互时, 产生的transition数据序列是在时间上高度关联(correlated)的, 如果这些数据序列直接用于训练, 会导致神经网络的overfit, 不易收敛。DDPG的actor将transition数据先存入experience replay buffer, 然后在训练时, 从experience replay buffer中随机采样mini-batch数据, 这样采样得到的数据可以认为是无关联的。
            \item target 网络和online 网络的使用,  使的学习过程更加稳定, 收敛更有保障。
        \end{enumerate}
    
        \textbf{参考文献}: [1] Lillicrap T P ,  Hunt J J ,  Pritzel A , et al. Continuous control with deep reinforcement learning[J]. Computer ence, 2015.
        
   
\section{例子}
    \subsection{围棋}
        \subsubsection{规则}
            \begin{itemize}
            \item  \textbf{基本元素}:
                \begin{itemize}
                    \item \textbf{棋盘}: 19×19等距且垂直交叉的平行线网格, 共391个交叉点.
                    \item \textbf{棋子}: 黑白两色, 对局双方各执一色, 空棋盘开局, 黑先白后, 交替下一子在棋盘空白点上, 下定后不得移动.
                    \item \textbf{气}: 棋子紧邻的空点(上下左右). 紧邻同色子连成一个整体, 气一并计算且一空点只算一次.
                \end{itemize}
            
            \item  \textbf{基本规则}:
                \begin{enumerate}
                    \item \textbf{无气提子}: 若棋子无气, 应立即提出棋盘. 下子后，若对方或双方棋子无气, 都只提对方无气子.
                    \item \textbf{无气禁入}: 棋盘上空点, 若下子后，该子立即无气且同时不能提对方棋子, 则该空点禁止下子.
                    \item \textbf{禁止全局同形}: 下子后, 不得使对方重复面临曾出现过的棋局. (全局同形再现是妨碍终局的唯一技术性原因, 应禁止.)
                    \item \textbf{终局}: 双方交替放弃下子. 双方除了自己的禁入点和眼点(对方禁入点且紧邻皆为自己), 均无空点可下时, 游戏结束.
                    \item \textbf{输赢判定}: 数子法, 死棋提出, 数棋盘上活棋(不能被提出的棋子)及其围住空点(紧邻皆为同色棋子)数. 黑色先手, 贴3.75子, 黑胜(黑棋数子数 - 184.25)子, 白胜(白棋数子数 - 176.75)子, 正赢负输.
                \end{enumerate}
                
            \item \textbf{性质}:
                \begin{itemize}
                    \item \textbf{劫争}: 双方棋子互围, 一方提子, 另一方不得立即回提, 只能在别处下子, 再回提, 以避免全局同形导致死循环.
                \end{itemize}
            \end{itemize}
\end{document}