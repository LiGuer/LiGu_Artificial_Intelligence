\documentclass{article} 
\usepackage{amsmath}
\usepackage[UTF8]{ctex}
\title{}\date{} \setlength{\parindent}{0pt} \linespread{1.25}
\usepackage{ntheorem}\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{geometry} \geometry{a4paper,left=1cm,right=1cm,top=1cm,bottom=1.5cm}
\begin{document}

\section{主成分分析}
		[目标]:
			数据降维，提取数据的主要特征分量, 满足：
			(1) 最近重构性: 样本点到该超平面的距离都足够近。
			(2) 最大可分性: 样本点在该超平面的投影尽可能分开。
			
		[优化问题]:
    		\begin{align*}
    			\min_W  &\quad	tr( W^T x x^T W )\\
    			s.t.    &\quad	W^T W = I
    		\end{align*}
			
		[流程]:
			(1) 数据中心化, Σ \vec x_i = 0
			(2) 计算协方差矩阵 C = X X^T
			(3) 对协方差矩阵C 特征值分解
			(4) 取最大d'个特征值所对应的特征向量{w1,w2,...,wd'},投影矩阵 W = (w1,w2,...,wd')
			(5) 样本点在超平面投影: y_i = W^T x_i
			
		[原理]:
				分别从目标(1, 2)可以推得同样的结果
			(3)	目标函数: 样本点到超空间投影 y = WT x 尽可能分开
				即.方差最大:max Σ WT x xT W
				协方差矩阵:
					D = 1/m Y Y^T = 1/m (PX) (PX)^T = 1/m P X X^T P^T = 1/m P C P^T
				协方差矩阵对角化
			(4) 优化问题构造:
					min_W		tr( W^T x x^T W )
					s.t.		W^T W = I
			(5) 计算最优点:
				Lagrange函数 L(W,λ) = W^T x x^T W + λ( W^T W - I )
				Lagrange对偶 G(λ) = inf L(W,λ) = inf (W^T x x^T W + λ( W^T W - I ))
				L(W,λ)求导, 当导数为0时, 取得极值
				=>	X X^T ω_i = λ_i ω_i,
				即.对协方差X XT, 特征值求解
			()	取特征值最大的yDim个特征向量, 即目标投影矩阵W


\section{K-Mean 聚类}
		[目标]:
			聚类. 对N维分布的数据点，可以将其聚类在 K 个关键簇内.
			
		[步骤]:
			(1) 随机选择 K 个簇心点 Center
			(2) 迭代开始
				(3) 归零 Cluster , Cluster: 簇,记录ith簇内的数据指针。
				(4) 计算每个xi到簇心μj的距离
					(5) 选择距离最小的簇心, 将该点加入其簇内
				(6) 对每个簇,计算其质心 Center'
				(7) Center≠Center' , 则更正Center为 Center'
				(8) 迭代重新开始
			(9) 一轮无更正时，迭代结束

\section{最小二乘法}
		[目标]:
			求一条直线，使得所有样本点到该直线的Euclid距离最小.
			
		[优化问题]:
    		\begin{align*}
    			\min_w  &\quad	MSE(\tilde y) · n = \sum (\tilde y - y)^2 = \sum (w^T x - y)^2
    		\end{align*}
		[原理]:
			(1) 直线方程: f(x) = w^T x + b = w^T x'		(x' = [1, x], w_0 = b)
			(2) 均方误差:
					MSE(y~)*n = Σ (y~ - y)²
					= Σ (w^T x - y)² = (X w - y)^T (X w - y)
			(3) 优化问题构造: 无约束凸优化问题
					min_W	Σ (w^T x - y)² = (X w - y)^T (X w - y)
			(4) 计算最优点: 求导, 导数为0时取得极值
					∂MSE / ∂W = 2·X^T·(w^T x - y) = 0
				=>	w* = (X^T X)^-1 X^T y
					f(x) = x^T (X^T X)^-1 X^T y
			(5) 一维场景:
				优化问题:
					min_{w,b}	Σ (w^T x_i + b - y_i)²
				=>	∂E/∂w = 2( w^T Σx_i² + Σ(x_i(b - y_i)) )	= 0
					∂E/∂b = 2( n b + Σ(y_i - w x_i) )			= 0
				最优点:
				=>	wΣx_i² = Σ_i x_i y_i - 1/n (Σ_i x_i)(Σ_i y_i) + w/n (Σ_i x_i)²
					w* = (Σ y_i(x_i - x¯)) / (Σ x_i² - 1/n (Σ x_i)²)
					b* = 1/n·Σ(y_i - w x_i)
					
\section{支持向量机}
		[目标]: 找到目标超平面, 使得所有样本点间隔最小值$\gamma_{min} = \min \gamma_i$最大.
		
		[优化问题]: 凸二次规划问题
		    \begin{align*}
		        min     &\quad	\frac{||w||^2}{2}\\
				s.t.    &\quad	y_i (w^T x_i + b) \le 1
		    \end{align*}

		[步骤]:
            \begin{enumerate}
                \item 计算核矩阵
                \item 计算$\lambda^*$
                \begin{enumerate}
                    \item 选择 i
                    \item 选择 j
                    \item 计算 $K_{ii}+K_{jj}-2K_{ij}, L, H$
                    \item 更新 $\lambda_j, \lambda_i$
                    \item 更新 b
                \end{enumerate}
                \item 计算 w*, b*
            \end{enumerate}

		[原理]:
			(1) 超平面方程: $w^T x + b = 0$\\
				点面距: $d = \frac{|w^T x + b|}{||w||}$
				
			(2) 分类
			\begin{align*}\left \{ \begin{array}{cc}
				w^T x_i + b \ge +1    &\quad (y_i = +1)\\
				w^T x_i + b \le -1	&\quad (y_i = -1)\\
			    \end{array} \right.
				\quad\Rightarrow\quad y_i (w^T x_i + b) \ge 1
			\end{align*}

			(3) 间隔: 离超平面最近的2个异类样本点到超平面的距离之和.
					$$\Delta = \frac{2}{||w||}$$
					
			(4) 优化问题构造: 使间隔最大化. 凸二次规划问题
					max  2 / ||w||    <=>    min  ||w||² / 2
				=>	min		||w||² / 2
					s.t.	y_i (w^T x_i + b) ≥ 1
			(5) 计算最优点:
				Lagrange函数
					L(w, b,λ) = ||w||² / 2 + Σλ_i (1 - y_i (w^T x_i + b))
					L(w, b,λ)求导, 当导数为0时, 取得极值
					=>	w* = Σ λ_i y_i x_i		0 = Σ λ_i y_i
				Lagrange对偶
					G(λ) = Lx(w*, b*, λ) = Σλ_i - 1/2 Σ_i Σ_j λ_i λ_j y_i y_j x_i^T x_j
				对偶问题: 二次规划问题
					max		G(λ)
					s.t.	λ≥0		Σ λ_i y_i = 0
					=>	Sequential Minimal Optimization算法求解λ*
				KKT条件
					λ≥0    y_i (w^T x_i + b) - 1 ≥ 0    λ_i(y_i(w^T x_i + b) - 1) = 0
			(6) 核函数:将样本从原始空间映射到更高维特征空间,使得其线性可分.
				=>	超平面方程: w^T Φ(x) + b = 0
					min		||w||² / 2
					s.t.	y_i (w^T Φ(x_i) + b) ≥ 1
					G(λ) = Σλ_i - 1/2 Σ_i Σ_j λ_i λ_j y_i y_j Φ(x_i)^T Φ(x_j)
				=>	设 核函数к(x_i, x_j) = Φ(x_i)^T Φ(x_j)
					f(x) = w*^T x + b = Σ λ_i y_i к(x, x_i) + b
				*	к是核函数 <=> 核矩阵[a_ij = к(x_i, x_j)]总是半正定
			(7) Sequential Minimal Optimization算法
				∵	λ是n-1自由度, 确定前n-1个量, 则第n个由Σ λ_i y_i = 0自动确定.
				∴	每次选2个λ_i λ_j, 固定其他λ_k不变, 优化λ_i λ_j, 更新b
				优化λ_i λ_j:
				=>	λ_i = (-Σ_{k≠i≠j} λ_k y_k)·y_1 - λ_j y_i y_j = ζ y_1 - λ_j y_i y_j
					G(λ_j) = (λ_j + ζ y_1 - λ_j y_i y_j) + C - v_i (ζ-λ_j y_j) - v_j λ_j y_j
							- 1/2 к_ii (ζ-λ_j y_j)² - 1/2 к_jj λ_j² - к_ij λ_j y_j (ζ-λ_j y_j)²
					其中 v_i = Σ_{k≠i≠j} λ_i y_i к_ki = f(x_i) - λ_i y_i к_ii -λ_j y_j к_ij - b, v_j同理
					∂G/∂λ_j = -(к_ii+к_jj-2к_ij)(λ_j^old - λ_j^new) + y_j(y_j-y_i+f(x_i)-f(x_j)) = 0
				=>	λ_j^new = λ_j^old + y_j (E_i - E_j)/(к_ii+к_jj-2к_ij)		其中E_i = f(x_i) - y_i
				修剪: 使得λ_j* 满足约束条件, λ_j应当∈[L,H]:
					y_i≠y_j时,	下界L = max(0,λ_j^old-λ_i^old)	,上界L = min(C,λ_j^old-λ_i^old + C)
					y_i= y_j时,	下界L = max(0,λ_j^old+λ_i^old - C),上界L = min(C,λ_j^old+λ_i^old)
				更新b:
			(8)	软间隔:

\section{神经网络}
    \subsection{前馈神经网络}
        \subsubsection{正向传播}
            $\sigma()$: 激活函数, 使线性拟合非线性化, eg. relu(x), Sigmoid(x)
            $$\boldsymbol y = \sigma (\boldsymbol w \boldsymbol x + \boldsymbol b)$$


            误差·损失函数: $E_{total} = \sum (target_i - out_i)^2$
            $$E(\boldsymbol{w})=\frac{1}{2} \sum_{n=1}^{N}\left\|\boldsymbol{y}\left(\boldsymbol{x}_{n}, \boldsymbol{w}\right)-\boldsymbol{t}_{n}\right\|^{2}$$

        \subsubsection{反向传播 --- 梯度下降最优化}
    		$$\boldsymbol{w}^{(\tau+1)}=\boldsymbol{w}^{(\tau)}-\eta \nabla E\left(\boldsymbol{w}^{(\tau)}\right)$$
    		
    		设$z = \boldsymbol w^T \boldsymbol x + \boldsymbol b$
            $$\left \{ \begin{array}{lr}
                \delta_l = (\boldsymbol w_{l+1}^T·\delta_{l+1})·\sigma'(z_l) & \text{(每层误差)}\\
                \delta_L = \nabla E · \sigma'(z_{outl}) & \text{(输出层误差)}\\
                \nabla E(\boldsymbol w_l) = \delta_l \boldsymbol x_l^T & \text{(每层参数误差)}
            \end{array} \right.$$

            \textbf{证明}:
        		
        		每层参数误差, 由链式法则得(其中$L$指输出层): 
        		    \begin{align*}
        		        \frac{\partial E}{\partial w_l} 
        		        &= \frac{\partial z_l}{\partial w_l} \left(\frac{\partial y_l}{\partial z_l}\frac{\partial z_l}{\partial y_{l+1}}\right) ... \left(\frac{\partial y_{L-1}}{\partial z_{L-1}} \frac{\partial z_{L-1}}{\partial y_L} \right) \frac{\partial y_L}{\partial z_L} \frac{\partial E}{\partial y_L}\\
        		        &= \frac{\partial z_l}{\partial w_l}  \left(\prod_{i = l}^{L} \frac{\partial y_i}{\partial z_i}\frac{\partial z_i}{\partial y_{i+1}} \right) \frac{\partial y_L}{\partial z_L} \frac{\partial E}{\partial y_L}
        		    \end{align*}
        		    
        	    其中:
        		    \begin{align*}
        		        \frac{\partial y_l}{\partial z_l} &= \sigma'(z_l)\\
        		        \frac{\partial z_l}{\partial y_{l+1}} &= \boldsymbol w_{l+1}^T
        		    \end{align*}
        		    
        		令$\delta_l$为每层误差:
        		    \begin{align*}
        		        \delta_l &=\left(\prod_{i = l}^{L} \frac{\partial y_i}{\partial z_i}\frac{\partial z_i}{\partial y_{i+1}} \right) \frac{\partial y_L}{\partial z_L} \frac{\partial E}{\partial y_L}\\
        		        &= \left(\prod_{i = l}^{L} \sigma'(z_i)\boldsymbol w_{i+1}^T\right) \sigma '(z_L)\frac{\partial E}{\partial y_L}
        		    \end{align*}
        		    
        		得到每层参数误差结果:
        		    \begin{align*}
        		        \Rightarrow \frac{\partial E}{\partial w_l} &= \delta_l \frac{\partial z_l}{\partial w_l} = \delta_l x_l^T\\
        		        \delta_L &= \frac{\partial E}{\partial y_L} ·\sigma '(z_L)
        		    \end{align*}
        		    
            \textbf{附}:
            
        		激活函数的导函数:
        		    \begin{align*}
        		        relu(x) &= \max(0, x)\\
        		        relu'(x) &= x > 0 ? 1 : 0 = step(x) \quad \text{阶跃函数}\\
        		        sigmoid(x) &= \frac{1}{1+e^{-x}}\\
        		        sigmoid'(x) &= sigmoid(x) · (1 - sigmoid(x))
        		    \end{align*}
\end{document}