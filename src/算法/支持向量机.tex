* 支持向量机
	\def{支持向量机}
		支持向量机是一种二分类模型, 目的是找到一个超平面, 使得超平面与任意样本点之间的最小距离最大. 约束条件是超平面能够使得所有样本点正确分类. 支持向量机的优化问题形式(凸二次规划问题)如下. 
		$
			\arg\max_{\. w, b} &\quad \min_i d_i = \frac{\min_i\ |\. w^T \. x_i + b |}{||\. w||}
			s.t. &\quad y_i (\. w^T \. x_i + b) ≤ 1
		$
		$
			=> \arg\max_{\. w, b}	 &\quad	\frac{||\. w||^2}{2}
			s.t.	&\quad	y_i (\. w^T \. x_i + b) ≤ 1
		$
	
		\proof		
			超平面方程: $\. w^T \. x + b = 0$, 点面距: $d = \frac{|\. w^T \. x + b|}{||w||}$
			
			分类
				$ \{\mb
					\. w^T \. x_i + b ≥ +1	&\quad (y_i = +1)
					\. w^T \. x_i + b ≤ -1	&\quad (y_i = -1)
					\me\right.
					=> y_i (\. w^T \. x_i + b) ≥ 1
				$

			间隔: 离超平面最近的2个异类样本点到超平面的距离之和.
				$Δ = \frac{2}{||w||} \tag{间隔}$
						
			优化问题构造: 使间隔最大化. 凸二次规划问题
				$\max\quad  \frac{2}{||\. w||} \quad => \quad \min\quad  \frac{||\. w||^2}{2}$
				$s.t.\quad	y_i (\. w^T \. x_i + b) ≥ 1$
	
	\bf{解}, 解凸优化问题, 计算最优点: 先求Lagrange函数, Lagrange函数对$\. w, b$求导, 令导数为0取得极值$\. w^*, b^*$, 再求Lagrange对偶函数
		$
			L(\. w, b,\. \lambda) = \frac{||\. w||^2}{2} + \sum_i \lambda_i (1 - y_i (\. w^T \. x_i + b)) \tag{Lagrange函数}
		$
		$
			=> \{\begin{array}{rl} \. w^* &= \sum_i \lambda_i y_i x_i\\ 0 &= \sum_i \lambda_i y_i \end{array}.\tag{$\. w, b$极值}
		$
		$G(\. \lambda) = L(\. w^*, b^*, \. \lambda) = \sum \lambda_i - \frac{1}{2} \sum_i \sum_j \lambda_i \lambda_j y_i y_j \. x_i^T \. x_j \tag{Lagrange对偶}
		$

		得到对偶问题, 为二次规划问题形式, 可利用Sequential Minimal Optimization算法求解得到$\. \lambda^*$
			$
				\max_{\. \lambda} &\quad G(\. \lambda)
				s.t. &  \quad \lambda_i ≥ 0
					& \quad \sum_i \lambda_i y_i = 0
			$
			
		其中, KKT条件如下
			$
				\lambda &≥ 0
				y_i (\. w^T \. x_i + b) - 1 &≥ 0
				\lambda_i(y_i(\. w^T \. x_i + b) - 1) &= 0
			$
					
	* 步骤:
		* 计算核矩阵
		* 计算$\lambda^*$
		
			* 选择 i
			* 选择 j
			* 计算 $K_{ii}+K_{jj}-2K_{ij}, L, H$
			* 更新 $\lambda_j, \lambda_i$
			* 更新 b
		
		* 计算 w*, b*

* 核方法
	\def{核方法}
		核方法通过核函数将样本从原始空间映射到更高维特征空间,使得其线性可分. 其中, 核函数定义为, 将原始空间中的向量作为输入向量, 并返回特征空间(转换后的数据空间,可能是高维)中向量的点积的函数称为核函数. 使用内核, 不需要显式地将数据嵌入到空间中, 因为许多算法只需要图像向量之间的内积(内积是标量); 在特征空间不需要数据的坐标. $\kappa(\. x_i, \. x_j) = \phi(\. x_i)^T \phi(\. x_j)$

		对偶公式: 将正则化的平⽅和误差函数完全由核函数表示, 因此我们可以直接针对核函数进⾏计算, 避免了显式地引⼊特征向	量$\phi(\. x)$, 这使得我们可以隐式地使⽤⾼维特征空间, 甚⾄⽆限维特征空间
		$
			J(\. a)=\frac{1}{2} \. a^T \. {K} \. {K} \. a-\. a^T \. {K} \. t+\frac{1}{2} \. t^T \. t+\frac{\lambda}{2} \. a^T \. {K} \. a
		$

		Gram矩阵:
		$
			\. K = \. \Phi \. \Phi^T = \left[\begin{array}{ccccc}
			\kappa(\. x_1, \. x_1) & ·s & \kappa(\. x_1, \. x_j) & ·s & \kappa(\. x_1, \. x_m) 
			\vdots & \ddots & \vdots & \ddots & \vdots 
			\kappa(\. x_i, \. x_1) & ·s & \kappa(\. x_i, \. x_j) & ·s & \kappa(\. x_i, \. x_m) 
			\vdots & \ddots & \vdots & \ddots & \vdots 
			\kappa(\. x_m, \. x_1) & ·s & \kappa(\. x_m, \. x_j) & ·s & \kappa(\. x_m, \. x_m)
			\end{array}\right]
		$

	\Example
		* 线性核: $\kappa(\. x_i, \. x_j) = \. x_i^T \. x_j$
		* 高斯核: $\kappa(\. x_i,\. x_j) = e^{-\frac{1}{2}(\. x_i - \. x_j)^T \Sigma^{-1} (\. x_i - \. x_j)}$
		* 多项式核: $\kappa(\. x_i, \. x_j) = (\. x_i^T \. x_j)^{d}$
		* 拉普拉斯核: $\kappa(\. x_i, \. x_j) = e^{-\frac{\|\. x_i-\. x_j\|}{\sigma}}$
		* Sigmoid核: $\kappa(\. x_i, \. x_j)=\tanh (\beta \. x_i^T \. x_j+\theta)$